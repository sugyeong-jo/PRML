# 9.4 The EM Algorithm in General

**Goal**: Find the maximum likelihood solutions for models with latent variables.

### Step 0: Consider a probabilistic model
$$p({\bf X}, {\bf Z}|{\bf \theta})$$
- ${\bf X}$: observed variables
- ${\bf Z}$: hidden variables
- ${\bf \theta}$: a set of parameters


### Step 1: Set the likelihood function
$$p({\bf X}|{\bf \theta}) = \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) \quad(9.69)$$
${\bf Z}$ could be continuous variables with ingegration. The goal is to maximize the likelihood function.

Due to the *Jensen's Inequality* , (9.69) is described as followed.

*Jensen's Inequality*: $E[f(x)] \ge f(E[x])$ 
$$p({\bf X}|{\bf \theta}) = \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) \to  \ln \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) = \ln \sum_{\bf Z} q({\bf Z}){{p({\bf X}, {\bf Z}|{\bf \theta})}\over q({\bf Z})}\ge \sum_{\bf Z} q({\bf Z}) \ln {{p({\bf X}, {\bf Z}|{\bf \theta})}\over q({\bf Z})}$$

Here, 
$$\sum_{\bf Z} q({\bf Z}) \ln {{p({\bf X}, {\bf Z}|{\bf \theta})}\over q({\bf Z})}\equiv {\mathcal L(q,\theta)} \text{ or } {\mathcal F(q,\theta)} $$

- ${\mathcal L(q,\theta)}$ : Functional 
-  ${\mathcal F(q,\theta)}$ : ?



##
Using the follow process,
$$p({\bf X}, {\bf Z}|{\bf \theta}) = {{{p({\bf X,Z,\theta}})}\over{p(\bf \theta)}}
= {{{p({\bf Z|X,\theta}})p({\bf X|\theta})p(\theta)}\over{p(\bf \theta)} } 
= {p({\bf Z|X,\theta}})p({\bf X|\theta})$$
The ${\mathcal L(q,\theta)}$ is equal as

$${\mathcal L(q,\theta)} = \sum_{\bf Z} q({\bf Z}) \ln {{p({\bf X}, {\bf Z}|{\bf \theta})}\over q({\bf Z})}= \sum_{\bf Z} q({\bf Z}) \ln {{p({\bf Z}|{\bf X, \theta})p(\bf X|\theta)}\over q({\bf Z})} \\\ 
= -\sum_{\bf Z}  q({\bf Z}) \ln {q({\bf Z})\over {{p({\bf Z}|{\bf X, \theta})}}} + \sum_{\bf Z} q({\bf Z}) \ln {{p({\bf X}|{\bf \theta})}}\\\ 
=- KL(q({\bf Z})\|p({\bf Z|X,\theta}))+\ln p({\bf X}|{\bf \theta}) $$ 

Therefore,  $p({\bf X}|{\bf \theta})\ge {\mathcal L(q,\theta)}$ and 
$$\ln p({\bf X}|{\bf \theta})={\mathcal L(q,\theta)}+KL(q\|p) \quad(9.70)$$

- ${\mathcal L(q, {\bf \theta})} = \sum_{\bf Z}q({\bf Z})\ln \left\{\frac{p({\bf X}, {\bf Z}|{\bf \theta})}{q({\bf Z})}\right\}\quad(9.71)$
 
- $KL(q\|p) = -\sum_{\bf Z} q({\bf Z}) \ln \left\{\frac{p({\bf Z}|{\bf X}, {\bf \theta})}{q({\bf Z})}\right\}\quad(9.72)$

##
The ${\mathcal F(q,\theta)}$ is equal as

$${\mathcal F(q,\theta)} = \sum_{\bf Z} q({\bf Z}) \ln {{p({\bf X}, {\bf Z}|{\bf \theta})}\over q({\bf Z})}= -\sum_{\bf Z} q({\bf Z}) \ln  q({\bf Z}) + \sum_{\bf Z} q({\bf Z})\ln {p({\bf X,Z | \theta})} \\\ 
= H(q) + \sum_{\bf Z} q({\bf Z})\ln {p({\bf X,Z | \theta})} $$

Therefore,  $p({\bf X}|{\bf \theta})\ge {\mathcal F(q,\theta)}$ and 
$$p({\bf X}|{\bf \theta})\ge\sum_{\bf Z} q({\bf Z})\ln {p({\bf X,Z | \theta})}+H(q)$$
![enter image description here](https://lh3.googleusercontent.com/4OtE8HKhX7pxDUK-0EX4i9sZBmZ_XFco3c6h3aY2n0esMggcd4mPnWovG7INWpPsJGefyjl1oLxDuHgM1AsxdR9NUpcCzy_abJbL8ppPEts9Yk5gZK44PvyDXThlT3kr5dJatt-YQuh-4wHg1kRhilOa_KIEl5AoRoNSXnbBMUm4m7Zpl4ZPleX-ofeDs07j_KsOl1S3_Tskt4eROBET_vUm37sg4PIzgzaPVA5FP42LRSuL-cqYv9p8pP7GHtFpfRTKum4nV81bNAgDm7mvImgI52HindTt8xS7PMAqUf63WqeDM-I-Gcu_LO5qrk7St89Zw1wurW2s0nIHiLkUXq4uhJTS_hgOqeMZQUTkJZ7ByfgAcQ_xe0dqMWv-2nnj1o6s_A6Bj0v2ydrufg25UR5TMytSo-YD0I1Acy5tUZ5R9bij6Z9xMQ0YzCHVitc89sFfKnf13CsWB2B-3lDwUaZdnFD93eC23eojLMtE9fwQJkTATLwcN5vPuF6HbPDyGsJSc7PA2I3EDbf3jJtVjz6Xh81eMkQx5Fby3RZp6G1wrdF2CcIyn70S4FUXdfQdyvtkyc2AGJ8occF8f7N3Iuo6M2GCB3tEha74hNNbY9uGY8vAbECgTVIvoW9TiQctNFSOLv4FgVvRDL2UMxdiXUN6DN_WCflNSlqA3FdA1FsL4SsTp2OIjw=w1577-h487-no)
##

### Step 3: (E - step) Maximize ${\mathcal L(q,\theta)}$
$$\ln p({\bf X}|{\bf \theta}) = L(q, {\bf \theta}) + KL(q\|p)$$

*(Gibb's Inequality)*:  함수 $q$와 $p$에 대해 $KL[p\|q] \ge 0$을 항상 만족함

- if $p=q \to KL[p\|q] \ge 0$, $p\ne q \to KL[p\|q] \ge 0$

- (proo) $KL[p\|q] = \sum_i p_i \ln{\frac{p_i}{q_i}} = - \sum_i p_i \ln{\frac{q_i}{p_i}}\ge -\ln[\sum_i p_i\frac{q_i}{p_i}]=-\ln[\sum_i q_i]=0$

$$\therefore{\mathcal L(q,\theta)} \text{ is a lower bound on }\ln p({\bf X}|{\bf \theta})$$

Suppose that the current value of the parameter vector is $\theta^{old}$. 
The lower bound ${\mathcal L(q,\theta^{old})}$ is maximized with respect to $q(\bf Z)$ while holding $\theta^{old}$.

The solution of $\max {\mathcal L(q,\theta^{old})}$ will occur when KL is vanished, 
$$ q^{k+1}({\bf Z}) = {p({\bf Z}|{\bf X}, {\bf \theta^{t}})}.$$
즉, 잠재변수에 대한 분포 $q({\bf Z})$가 사후분포와 동일할 때 ${\mathcal L(q,\theta^{old})}$ 가 가장 큰 값을 가지게 된다.
![enter image description here](https://lh3.googleusercontent.com/UhPqWR9SwqA4u1J_68uedNpx8ptjZZLodVNbJVMSTzULPmUrumyvirFdVkv3nOLJ2UPWQAAwkYAhqCMa-2a4J-m6Gr57CcG94cReOUqlGMvTNZqsbx9BLM5qsqEmSk2GDBO6O9qmNHee-XPPCbudPR9UjX6LdKYiSFrIacPKo6J6YrCSuO61EAmpGuVeKRsBLzwSw3johkJp7IN2-UyxAu-9cJT6dx22vzBDcqvZkjtXIwHJOgTANezi0UT8FfDXYW5yvQpmnFFwHKDJM23yVP82L83VIF3lvWljwoYecgcpWhZeRls9HbuCIKaucG-1qEheQVhgRRz1WAyDuym_wm0j3MNIQo-RZclwLpx1sUcOIXXftsP-5bg7lMD2zQMIaB9b8UJWelbsa4ZBUMCoNa3vYr93hI1qIr5_fjArudXT6ZU0RGNHTjcL4quWR8hWym05MHHj6iYP6qcUtuttaXzZB-Qd4Qe2SU9fkTsPqDY1gW75Bi6JXS8zBhQHatQtaO5_-UddzDiclKoy3zKXOROPzWm3W1GwgTWHAMtz5KrIIhLCOmxLyie7LaOf-U0AvsloHv4v1JKdFXFZiKeXnw6QlxVLUAYEldaYKdpA6K6nELcXAGe8Xm-YuWY57RcbZzbPJh-OvLVu2Xzg1IxfvPE-k0B-LQ8096g82ZMNbVpxsOGFQ0xd3Q=w1585-h515-no)

##

### Step 4: (M - step) Maximize ${\mathcal F(q,\theta)}$

The distribution $q(\bf Z)$ is held fixed and the lower bound $\mathcal L(q, θ)$ is maximized with respect to $\theta$ to give some new value $\theta^{new}$

Then the 
$$\theta ^{k+1} = \max {\mathcal F (q^{k+1},\theta)} = \max  \sum_{\bf Z} q^{k+1}({\bf Z})\ln {p({\bf X,Z | \theta})}$$
${\mathcal F(q,\theta)} =  \sum_{\bf Z} p({\bf Z}|{\bf X}, \theta^{old})\ln p({\bf X}, {\bf Z}|\theta) - \sum_{\bf Z} p({\bf Z}|{\bf X}, \theta^{old})\ln p({\bf Z}|{\bf X}, \theta^{old})\\\ = Q(\theta, \theta^{old}) + const \quad(9.74)$

![enter image description here](https://lh3.googleusercontent.com/HOSdorQeSv6bsvoczJ5g1mFx1t-PG26Q7VQ3mRQysxRcHogU1GGjegbdyR_gQ6x2JIlKjVteuK34V0ixuYMBfwUNsf1KGUwJG1GTNbZsoNxpXRrBadnmH6VvEalnvymB01M7ATUOysgJOOaXiwHyyvSkSCd3jzF-njiugrgWVywKpPHjDHPkHYwBDTEnMFBX1mSdwhnd7WXuGJIX7c1LM1oR-PUuPwBYVscTm6_-G-AbYp_Rnl8xj9ppSaW7Zh7Sl5D_OtU0o94l4Y4sffX43LnuDicfDUIBXNnCpn8PvlX9qi8hubxkKVMbu8YK5pxzJK76sUlXNDD6dJUQSIk224jcNyLVUh_bvmpqx8kDOQqf3zPWsFlSNHMs9f0SGYP4WCnbV_mAZp6Cua6kGoCt3QnkxNIJEAVhueRBcndgtOf-uTwkv-WrXwBifUbDzCywn1dfaXEPX7MBvtkaoZiBMFSgPJjll8bqMDB5AUj-qs04SmPK2RohMv61w-_X_HWqhqGjoDAbhYcO6InNlug7bjsp7KWtAoFwqspgTfG6ZlGMHimMEqpnU8EG4QUFTRJQSJtGIKnUnKJzr1j5-104cbSPX00tjjmpjpHbgtBiheCEn3w4cLeddQ6_6JYSttgXU6gQ7P-Rg33Dj9MfBMQS47Ng3aAq_Zgc3lY66_YV-7zJ2s6KCP-bvw=w1568-h642-no)

### Step5: Iteration about E-step and M-step until convergence 
##

### Sumary

![enter image description here](https://lh3.googleusercontent.com/QQKFbCAFBoe1Oafe6ub8i6buEMfbp1VzZSZDsTRd8F9tul2aORiWE2AXp8uNoDnQ7_xaN70wCmmhIZuO9wXlFhnNE-1WrtWKjnfy6BUZX3aW7VTLzyjyMbG1YSPnaZnb_wg0BrWym2Luj3blJX1iiIHlQXEDsZMmdaCadUCHuICS3E2i79zbWSmBXBrGoc1o4Bs0dZ21CzBWrto-f3LqVWF03JhWVvUOLl2pfOUdSn0PmWCpwcGOO4ghfpSfc3WP4lkCEtLYH4oSfaq1u_y-q3qk4iBPvbEmJ-kfhBcZDlFhgebN0exbHM-UXApwDIwe1t-6fFykPW8gn4os_pRptanlx-5hBV5qhub89wLOp2s84of_4O_cS8fNwa6sl7I4Pfe9LUrmsZ6hnkeTD1NbBsOYcqsSebxMzRzm_AFJbimWarg2jCc5n89NdE3Ajh6rz6gaCKEEebUyAkAKf2ydZgF_-H84Ou1v0KNsCRBnU3705tK95zxw4hBQp0R5lQ3alUCVaYhk_RZCEjyQa8LKjwaIdFvUa7V0SBlpKGhbbp8ptghxlDpqY2NCqrChhRcc_w1HypD232JV4orBiRGN7E1d8JRcoYSqDaxJFoK3mu76ZGoOqYtfeVRUQHppBeYPBIyJW7Yk0mruX-oPgnEZEKoXzovb5cow2cIkh-e8OFK0bDmewxvsKA=w1572-h586-no)
**E-step**: the posterior probability that is evaluated in the E step
$$p({\bf Z}|{\bf X}, \theta) = \frac{p({\bf X}, {\bf Z}|\theta)}{\sum_{\bf Z}p({\bf X
}, {\bf Z}|\theta)}=\frac{\prod_{n=1}^N p({\bf x}_n, {\bf z}_n|\theta)}{\sum_{\bf Z}\prod_{n=1}^N p({\bf x}_n, {\bf z}_n|\theta)}=\prod_{n=1}^N p({\bf z}_n|{\bf x}_n, \theta)\quad(9.75)$$


**M-step**
$${\bf \mu}_k^{new} = {\bf \mu}_k^{old} + \left(\frac{\gamma^{new}(z_{mk})-\gamma^{old}(z_{mk})}{N_k^{new}}\right)({\bf x}_n-{\bf \mu}_k^{old})\quad(9.78)$$
$$N_k^{new} = N_k^{old} + \gamma^{new}(z_{mk}) - \gamma^{old}(z_{mk})\quad(9.79)$$




# 9.3.  An Alternative View of EM

## 9.3.1 Gaussian Mixtures revisited
![enter image description here](https://lh3.googleusercontent.com/hIj0rVb2FZx2rQ49Dzntrdgus0V17shtV8PCIioZMDMtavKS8r_rF8ar2Az6cL8Z9JO2RR3SvNGiTpBehnBPDrWlmQQttrd6FiByecx2kOjliOwp-IwT_r84d-S-OD8VLz9-VzWw6TGuz7ALptdOka5LTog9ePmzTeby1GO4UYl0ijoEk34pvhb3AsBhsQ2Y57NlGKb0MaRmB-jGXNn_X8jo3zvWgAwXdI42aeZ95GeJDDkj68v79SXnS8HX6newp3E2rm9-OwkYCiOLZ-tZubAaZ7UHz9ZhmAZh4XTLw8DKsbQxQF-eLw2cgNgoMbX7h9ssT9tM6zAH-zuGlJ1ImYbOCpqsp5R-NTpTv-bnf9cDG0ouqyQCSVFzpBVrE3dkvz5bg_xb-rn0tw0iA0S4cI6xe1W2cRzqv5YN1h_B2zUme3vkeyRPlRmju8-hz-C3ArdLfqG_lzfzktqZsG6dt306FZrQav_NNEzno4OjgwxEpnPv6iX45B9CIsfn6SOvHT4DTiVsrwegcn_RLVWLCbdYj2yO21N2ibsjA3y11mzT1385ysE-t5I_5WGRIsQP29vPDeckhvneRb5cd8N0bpnG-sFDjPVmAJCXncxwQdXpTkiYmnBZgyFp-tGO02pnkTKgt6aI87LiQhamem46LJVAQeTOnNcp700YF3RX3KSP2PzTbT6knw=w1540-h382-no)

### Step 1: Set the likelihood function
$$p({\bf X}|{\bf \theta}) = \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) \quad(9.69)$$

$$likelihood =p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi})= \prod_{n=1}^N p({\bf x_n, z_n})=\prod_{n=1}^N p({\bf x_n| z_n})p({\bf z_n}) 
=\prod_{n=1}^N\prod_{k=1}^K \pi_k^{z_{nk}} N({\bf x}_n|{\bf \mu}_k, \Sigma_k)^{z_{nk}} \qquad{(9.35)}$$


$${\mathcal L}=\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi}) = \sum_{n=1}^N\sum_{k=1}^K z_{nk}\{\ln\pi_k + \ln N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\} \qquad{(9.36)}   $$


- $p({\bf x})= \sum_{\bf z}p({\bf z})p({\bf x|z})=\sum_{k=1}^K\pi_k {\mathcal N}{(\bf x|\mu_k, \Sigma_k)}\quad(9.12)$

- $p(z_k=1) = \pi_k= \frac{1}{N}\sum_{n=1}^N z_{nk} \qquad{(9.37)}$

- $\sum_{k=1}^K \pi_k = 1 \quad {(9.9)}$, $0 \le \pi_k \le 1 \quad{(9.8)}$
- $p({\bf z})=\prod_{k=1}^K \pi_k^{z_k} \quad{(9.10)}$: multinomial distribution k개 중 하나 선택
- $p({\bf x}|z_k=1) = N({\bf x}|{\bf \mu_k}, \Sigma_k) \\\ \to p({\bf x}|{\bf z}) = \prod_{k=1}^{K} N({\bf x}|{\bf \mu}_k, \Sigma_k)^{z_k} \qquad{(9.11)}$ : multivariate 

- $\gamma(z_k) \equiv p(z_k=1|{\bf x}) = \frac{p(z_k=1)p({\bf x}|z_k=1)}{\sum_j^K p(z_j=1)p({\bf x}|z_j=1)}=\frac{\pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}|{\bf \mu}_j, \sigma_j)} \qquad{(9.13)}$: x가 특정 cluster에 속할 확률(Baysian Network)




### Step 2: E-step
>$$ q^{k+1}({\bf Z}) = {p({\bf Z}|{\bf X}, {\bf \theta^{t}})}$$

사후분포는 다음의 형태를 가진다. 
$$p({\bf Z}|{\bf X}, {\bf \mu}, \Sigma, {\bf \pi}) \propto \prod_{n=1}^{N}\prod_{k=1}^{K}[\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)]^{z_{nk}} \qquad{(9.38)}$$
로그 가능도 함수의 최대화해를 구하기 위해 로그 가능도 잠재 변수의 사후 분포에 대한 기댓값을 고려해야 한다.


$$<{\mathcal L}>=E_{\bf Z}[\ln p({\bf X}, {\bf Z}|\theta)] = E_{\bf Z}[\sum_n\ln p({\bf x}_n, {\bf z}_n|\theta)] = \sum_n E_{\bf Z}[\ln p({\bf x}_n, {\bf z}_n|\theta)]\\\\
= \sum_{n=1}^N E_{\bf Z}[\ln (p({\bf z}_n) p({\bf x}_n|{\bf z}_n, \theta))] \\\ 
= \sum_{n=1}^N E_{\bf Z}[\ln[\prod_{k=1}^K (\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k))^{z_{nk}}]]\\\ 
=\sum_{n=1}^N \sum_{k=1}^K E_{z}[(z_{nk})\ln(\pi_k N({\bf x}|{\bf \mu}_k, \Sigma_k))] \\\ 
= \sum_{n=1}^N \sum_{k=1}^K E_{z}[z_{nk}] \ln(\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k))\\\\
=\sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk})\{\ln\pi_k+\ln N({\bf x}_n|{\bf \mu_k}, \Sigma_k)\} \\\ 
= \sum_{n=1}^N\sum_{k=1}^K \gamma(z_{nk})\{\ln\pi_k -{\frac D 2}\ln \sigma_k^2 -\frac 1 {2\sigma_j^2}\|x_n-\mu_k\|^2  \}$$ 




사후분포하에서의 지표변수 $E[z_{nk}]$의 기댓값은 다음과 같이 주어진다. 


$$E[z_{nk}] = p(z_{k}=1|{\bf x}_n)= \frac{p(z_k=1)p({\bf x}|z_k=1)}{\sum_j^K p(z_j=1)p({\bf x}|z_j=1)} \\\\ 
\frac{\sum_{ {\bf z}_n} z_{nk}\prod_{k'}[\pi_{k'} N({\bf x}_n|{\bf \mu}_{k'}, \Sigma_{k'})]^{z_{nk'}}}{\sum_{ {\bf z}_n} \prod_j [\pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)]^{z_{nj}}}\\\ 
= \frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)} = \gamma(z_{nk})$$


### Step 3: M-step

$\frac {\partial<{\mathcal L}>}{\partial \mu_j}=0$
$$\mu^{new} = \frac {\sum_{n=1}^Nr_{nk}x_k}{\sum_{n=1}^Nr_{nk}}$$

$\frac {\partial<{\mathcal L}>}{\partial \sigma^2_j}=0$
$$\sigma^{2new} = \frac1 D\frac {\sum_{n=1}^Nr_{nk}\|x_n-\mu_j^{new}\|^2}{\sum_{n=1}^Nr_{nk}}$$
$\frac {\partial<{\mathcal L}>}{\partial \pi_j}=0$
$$\pi^{new} = \frac 1 N {\sum_{n=1}^Nr_{nk}}$$

## 9.3.2 Relation to K-means
In MoG, assume the $\pi_j=\frac 1 k\quad,j=1,...,k$

### Step 1: Set the likelihood function
$$p({\bf X}|{\bf \theta}) = \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) \quad(9.69)$$

$$likelihood =p({\bf x}|{\bf \mu}_k, \Sigma_k) \\\ 
= \frac{1}{(2\pi )^{M/2}}\frac{1}{\Sigma_k^{1/2}}\exp\left\{-\frac{1}{2}({\bf x - \mu}_k)^{\bf T}\Sigma^{-1}({\bf x - \mu}_k)\right\} \\\ 
=\frac{1}{(2\pi e)^{M/2}}\exp\left\{-\frac{1}{2e}\|{\bf x}-{\bf \mu}_k\|^2\right\} \qquad{(9.41)}$$
when $\Sigma_k \to 0 \to e$

### Step 2: E-step
>$$ q^{k+1}({\bf Z}) = {p({\bf Z}|{\bf X}, {\bf \theta^{t}})}$$

$<{\mathcal L}>=E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi})] \rightarrow -\frac{1}{2}\sum_{n=1}^N\sum_{k=1}^K r_{nk}\|{\bf x}_n-{\bf \mu}_k\|^2+const$
$$\gamma(z_{nk})= p(z_{k}=1|{\bf x}_n)= \frac{p(z_k=1)p({\bf x}|z_k=1)}{\sum_j^K p(z_j=1)p({\bf x}|z_j=1)}  
=\frac{\pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)}{\sum_{j=1}^K \pi_j N({\bf x}_n|{\bf \mu}_j, \Sigma_j)}  
=\frac{\pi_k \exp\{-\|{\bf x}_n-{\bf \mu}_k\|^2/2e\}}{\sum_j \pi_j \exp\{-\|{\bf x}_n-{\bf \mu}_j\|^2/2e\}} $$

### Step 3: M-step




$\frac {\partial<{\mathcal L}>}{\partial \mu_k}=0$
$$\mu^{new} = \frac {\sum_{n=1}^Nr_{nk}x_k}{\sum_{n=1}^Nr_{nk}}$$

$\frac {\partial<{\mathcal L}>}{\partial \Sigma_k}=0$
$$\Sigma^{new} = \frac1 D\frac {\sum_{n=1}^Nr_{nk}\|x_n-\mu_j^{new}\|^2}{\sum_{n=1}^Nr_{nk}}$$

$\frac {\partial<{\mathcal L}>}{\partial \pi_k}=0$
$$\pi^{new} = \frac 1 N {\sum_{n=1}^Nr_{nk}}$$

## 9.3.3 Mixtures of Bernoulli distributions
### Step 0: 

Consider a set of D binary variables xi, where i = 1,...,D, each of which is governed by a Bernoulli distribution
$$p({\bf x}|{\bf \mu}) = \prod_{i=1}^D \mu_i^{x_i}(1-\mu_i)^{(1-x_i)}\quad(9.44)$$

- $E[{\bf x}] = {\bf \mu} \quad(9.45)$
- $cov[{\bf x}] = diag(\mu_i(1-\mu_i)) \quad(9.46)$

consider a finite mixture of these distributions

$$p({\bf x}|{\bf \mu}, {\bf \pi}) = \sum_{k=1}^K \pi_k p({\bf x}|{\bf \mu}_k)\quad(9.47)$$ 

$$p({\bf x}|{\bf \mu}_k) = \prod_{i=1}^D \mu_{ki}^{x_i}(1-{\bf \mu}_{ki})^{(1-x_i)}\quad(9.48)$$

- $E[{\bf x}] = \sum_{k=1}^K \pi_k{\bf \mu}_k\quad(9.49)$

- $cov[{\bf x}] = \sum_{k=1}^K \pi_k \{\Sigma_k+{\bf \mu}_k{\bf \mu}_k^T\}-E[{\bf x}]E[{\bf x}]^T\quad(9.50)$
- $\Sigma_k = diag\{\mu_{ki}(1-\mu_{ki})\}$

### Step 1: Set the likelihood function
$$p({\bf X}|{\bf \theta}) = \sum_{\bf Z}p({\bf X}, {\bf Z}|{\bf \theta}) \quad(9.69)$$


$$log-likelihood=\ln p({\bf X}|{\bf \mu}, {\bf \pi}) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k p({\bf x}_n|{\bf \mu}_k)\right\}\quad(9.51)$$

로그 내부에 또 다시 합산이 나타나므로 최대가능도 해는 닫힌 형태 해를 갖지 않는다. 따라서 잠재변수 z를 도입하여 로그 가능도 함수를 이용해야 한다.

$$likelihood =p({\bf X}, {\bf Z}|{\bf \mu}, {\bf \pi})= \prod_{n=1}^N p({\bf x_n, z_n})=\prod_{n=1}^N p({\bf x_n| z_n})p({\bf z_n}) = \prod_{n=1}^N p({\bf x}|{\bf z}, {\bf \mu})p({\bf z}|{\bf \pi})$$

$$\ln p({\bf X}, {\bf Z}|{\bf \mu}, {\bf \pi})=\sum_{n=1}^N\sum_{k=1}^K z_{nk}\left\{ \ln\pi_k + \sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]\right\}\quad(9.54)$$

- $p({\bf x}|{\bf z}, {\bf \mu})=\prod_{k=1}^K p({\bf x}|{\bf \mu}_k)^{z_k} \quad(9.52)$

- $p({\bf z}|{\bf \pi}) = \prod_{k=1}^K \pi_k^{z_k}\quad(9.53)$




### Step 2: E-step
>$$ q^{k+1}({\bf Z}) = {p({\bf Z}|{\bf X}, {\bf \theta^{t}})}$$

$$<{\mathcal L}>=E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, {\bf \pi})] = \sum_{n=1}^N\sum_{k=1}^K \gamma(z_{nk})\left\{\ln\pi_k + \sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]\right\} \quad(9.55)$$


$$\gamma(z_{nk})=E[z_{nk}] = \frac{\sum_{ {\bf z}_n} z_{nk}\prod_{k'}[\pi_{k'} p({\bf x}_n|{\bf \mu}_{k'})]^{z_{nk'}}}{\sum_{ {\bf z}_n} \prod_j [\pi_{j} p({\bf x}_n|{\bf \mu}_j)]^{z_{nj}}} = \frac{\pi_k p({\bf x}_n|{\bf \mu}_k)}{\sum_{j=1}^K \pi_j p({\bf x}_n|{\bf \mu}_j)}\quad(9.56)$$



### Step3: M-step

$\frac {\partial<{\mathcal L}>}{\partial \mu_k}=0$
$$\mu^{new} =\bar{\bf x}_k\quad(9.59) = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk}){\bf x}_n\quad(9.58)$$

$\sum_k \pi_k =1$을 지키기 위한 라그랑주 승수 도입
$$\pi_k ^{new}= \frac{N_k}{N}\quad(9.60)$$
- $N_k = \sum_{n=1}^N \gamma(z_{nk})$

## 9.3.4 EM for Bayesian linear regression
### Step 1: Set the likelihood function

$$\ln p({\bf t}, {\bf w}| \alpha, \beta) = \ln p({\bf t}|{\bf w}, \beta)+\ln p({\bf w}|\alpha)\quad(9.61)$$

- $p({\bf t}|{\bf X}, {\bf w}, \beta)=\prod_{n=1}^{N}N(t_n|{\bf w}^T\phi(x_n), \beta^{-1}) \quad{(3.10)}$

- $\ln{p({\bf t}|{\bf w}, \beta)} = \sum_{n=1}^{N}\ln{N(t_n|{\bf w}^T\phi(x_n), \beta^{-1})}=\dfrac{1}{2}\ln{\beta}-\dfrac{1}{2}\ln{2\pi}-\beta{E_D({\bf w})} \quad{(3.11)}$
- $p({\bf w}|\alpha) = N({\bf w}|0, \alpha^{-1}{\bf I})$


### Step 2: E-step

$$ p({\bf w|t,X,\alpha,\beta})={\mathcal N}({\bf w|m,S})\quad(7.81)$$

- ${\bf m = \beta \Sigma \Phi^T t}\quad(7.82)$ 

- ${\bf S = (\alpha I+\beta \Sigma \Phi^T \Phi)^{-1}}\quad(7.83)$ 

$$<{\mathcal L}>=E[\ln p({\bf t}, {\bf w}|\alpha, \beta)] = \frac{M}{2}\ln\left(\frac{\alpha}{2\pi}\right) - \frac{\alpha}{2}E[{\bf w}^T{\bf w}]+\frac{N}{2}\ln\left(\frac{\beta}{2\pi}\right) - \frac{\beta}{2}\sum_{n=1}^N E[(t_n-{\bf w}^T\phi_n)^2]$$


### Step 3: M-step

$\frac {\partial<{\mathcal L}>}{\partial \alpha}=0$
$$\alpha = \frac{M}{E[{\bf w}^T{\bf w}]}=\frac{M}{m_N^T m_N+Tr(S_N)}\quad(9.63)$$
$$\alpha^{-1} = \frac{1}{M}({\bf m}^T{\bf m}+Tr(S))\quad(9.67)$$
$$\beta^{-1} = \frac{1}{N}\sum_{n=1}^N {t_n-{\bf m}^T\phi({\bf x}_n)}^2\quad(9.68)$$

##

$\ln p({\bf X}|{\bf \mu}, \Sigma, {\bf \pi}) = \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k N({\bf x}_n|{\bf \mu}_k, \Sigma_k)\right\}$





$\ln p({\bf X}|{\bf \theta}) = \ln \left\{ \sum_{\bf Z} p({\bf X}, {\bf Z}|{\bf \theta}) \right\} \qquad{(9.29)}$

$E_Z[\ln p({\bf X}, {\bf Z}|\theta)] = \sum_{\bf Z} p({\bf Z}|{\bf X}, {\bf \theta})\ln p({\bf X}, {\bf Z}|{\bf \theta})$

$Q({\bf \theta}, {\bf \theta}^{old}) = \sum_{\bf Z} p({\bf Z}|{\bf X}, {\bf \theta}^{old}) \ln p({\bf X}, {\bf Z}|{\bf \theta}) \qquad{(9.30)}$

${\bf \theta}^{new} = \arg\max_{\theta} Q({\bf \theta}, {\bf \theta}^{old}) \qquad{(9.31)}$

${\bf \theta}^{new} = \arg\max_{\bf \theta} Q({\bf \theta}, {\bf \theta}^{old}) \qquad{(9.32)}$

$Q({\bf \theta}, {\bf \theta}^{old}) = \sum_{\bf Z}p({\bf Z}|{\bf X}, {\bf \theta}^{old})\ln p({\bf X}, {\bf Z}|{\bf \theta}) \qquad{(9.33)}$

${\bf \theta}^{old} \leftarrow {\bf \theta}^{new} \qquad{(9.34)}$


$Q({\bf \theta}, {\bf \theta}^{old})+\ln p({\bf \theta})$


...

$p({\bf z})=\prod_{k=1}^{K} \pi^{z_k}$
$p({\bf x}|{\bf z}) = \prod_{k=1}^{K} N({\bf x}|{\bf \mu}_k, \Sigma_k)^{z_k}$

$E_{\bf Z}[\ln p({\bf X}, {\bf Z}|{\bf \mu}, \Sigma, {\bf \pi})] = \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk})\{\ln\pi_k+\ln N({\bf x}_n|{\bf \mu_k}, \Sigma_k)\}$

$E[X+Y] = E[X]+E[Y]$
$p({\bf x}_n|{\bf z}_n) = \prod_{k=1}^K N({\bf x}_n|{\bf \mu}_k, \Sigma_k)^{z_nk}$



${\bf \mu}_k = \frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk}){\bf x}_n$
$\Sigma_k = \frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})({\bf x}_n-{\bf \mu}_k)({\bf x}_n-{\bf \mu}_k)^T$
$\pi_k = \frac{N_k}{N}$




